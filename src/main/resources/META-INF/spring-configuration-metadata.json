{
  "groups": [
    {
      "name": "chatgpt",
      "type": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties"
    }
  ],
  "properties": [
    {
      "name": "chatgpt.api-key",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "",
      "description": "apiKey of chatgpt. It can be generated in this link https://platform.openai.com/account/api-keys",
      "required": true
    },
    {
      "name": "chatgpt.model",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "text-davinci-003",
      "description": "ID of the model to use. You can see our Model overview (https://platform.openai.com/docs/models/overview) for descriptions of them."
    },
    {
      "name": "chatgpt.url",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "https://api.openai.com/v1/completions",
      "description": "The request url."
    },
    {
      "name": "chatgpt.max-tokens",
      "type": "java.lang.Integer",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": 500,
      "description": "The maximum number of tokens to generate in the completion.The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096)."
    },{
      "name": "chatgpt.temperature",
      "type": "java.lang.Double",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "1.0",
      "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.We generally recommend altering this or top_p but not both."
    },{
      "name": "chatgpt.top-p",
      "type": "java.lang.Double",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "1.0",
      "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or temperature but not both."
    },

    {
      "name": "chatgpt.multi.model",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.MultiChatProperties",
      "defaultValue": "gpt-3.5-turbo",
      "description": "ID of the model to use. See the model endpoint compatibility (https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API."
    },
    {
      "name": "chatgpt.multi.url",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.MultiChatProperties",
      "defaultValue": "https://api.openai.com/v1/chat/completions",
      "description": "The request url."
    },
    {
      "name": "chatgpt.multi.max-tokens",
      "type": "java.lang.Integer",
      "sourceType": "io.github.flashvayne.chatgpt.property.MultiChatProperties",
      "defaultValue": 500,
      "description": "The maximum number of tokens to generate in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model's context length."
    },{
      "name": "chatgpt.multi.temperature",
      "type": "java.lang.Double",
      "sourceType": "io.github.flashvayne.chatgpt.property.MultiChatProperties",
      "defaultValue": "1.0",
      "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.We generally recommend altering this or top_p but not both."
    },{
      "name": "chatgpt.multi.top-p",
      "type": "java.lang.Double",
      "sourceType": "io.github.flashvayne.chatgpt.property.MultiChatProperties",
      "defaultValue": "1.0",
      "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or temperature but not both."
    },{
      "name": "chatgpt.image.url",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.ImageProperties",
      "defaultValue": "https://api.openai.com/v1/images/generations",
      "description": "The request url."
    }
  ],
  "hints": []
}