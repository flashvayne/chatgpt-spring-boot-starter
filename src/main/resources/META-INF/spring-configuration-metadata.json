{
  "groups": [
    {
      "name": "chatgpt",
      "type": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties"
    }
  ],
  "properties": [
    {
      "name": "chatgpt.api-key",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "",
      "description": "apiKey of chatgpt. It can be generated in this link https://beta.openai.com/docs/quickstart/adjust-your-settings"
    },
    {
      "name": "chatgpt.model",
      "type": "java.lang.String",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "text-davinci-003",
      "description": "GPT-3 models can understand and generate natural language. We offer four main models with different levels of power suitable for different tasks. Davinci is the most capable model, and Ada is the fastest.."
    },
    {
      "name": "chatgpt.max-tokens",
      "type": "java.lang.Integer",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": 500,
      "description": "The maximum number of tokens to generate in the completion.The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096)."
    },{
      "name": "chatgpt.temperature",
      "type": "java.lang.Double",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "0.0",
      "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.We generally recommend altering this or top_p but not both."
    },{
      "name": "chatgpt.top-p",
      "type": "java.lang.Double",
      "sourceType": "io.github.flashvayne.chatgpt.property.ChatgptProperties",
      "defaultValue": "1.0",
      "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.We generally recommend altering this or temperature but not both."
    }
  ],
  "hints": []
}